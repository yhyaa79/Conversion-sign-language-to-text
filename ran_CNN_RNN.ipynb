{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video\n",
      "Folder created: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images\n",
      "Folder created: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/3_square_images\n",
      "Folder created: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/4_resized_images\n",
      "Folder already exists: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/yolov5/runs/train/weights/best.pt\n",
      "Folder already exists: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data\n",
      "Deleted folder: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/yolov5/runs/detect/exp\n",
      "Command Output: \n",
      "Command Error: /Users/yayhaeslami/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/yolov5/runs/train/weights/best.pt'], source=/Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4, data=run_CNN_RNN/yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=run_CNN_RNN/yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ğŸš€ v7.0-315-g892e8a82 Python-3.11.5 torch-2.3.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
      "video 1/1 (1/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 106.0ms\n",
      "video 1/1 (2/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 118.8ms\n",
      "video 1/1 (3/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 147.2ms\n",
      "video 1/1 (4/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 93.5ms\n",
      "video 1/1 (5/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 134.1ms\n",
      "video 1/1 (6/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 104.8ms\n",
      "video 1/1 (7/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 114.4ms\n",
      "video 1/1 (8/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 109.0ms\n",
      "video 1/1 (9/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 95.1ms\n",
      "video 1/1 (10/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 104.1ms\n",
      "video 1/1 (11/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 105.1ms\n",
      "video 1/1 (12/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 94.7ms\n",
      "video 1/1 (13/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 102.4ms\n",
      "video 1/1 (14/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 140.2ms\n",
      "video 1/1 (15/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 98.9ms\n",
      "video 1/1 (16/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 97.4ms\n",
      "video 1/1 (17/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 99.9ms\n",
      "video 1/1 (18/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 109.1ms\n",
      "video 1/1 (19/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 130.9ms\n",
      "video 1/1 (20/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 97.3ms\n",
      "video 1/1 (21/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 94.1ms\n",
      "video 1/1 (22/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 96.4ms\n",
      "video 1/1 (23/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 102.0ms\n",
      "video 1/1 (24/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 116.0ms\n",
      "video 1/1 (25/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 100.6ms\n",
      "video 1/1 (26/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 94.5ms\n",
      "video 1/1 (27/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 89.9ms\n",
      "video 1/1 (28/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 107.4ms\n",
      "video 1/1 (29/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 91.2ms\n",
      "video 1/1 (30/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 91.3ms\n",
      "video 1/1 (31/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 98.0ms\n",
      "video 1/1 (32/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 100.4ms\n",
      "video 1/1 (33/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 112.5ms\n",
      "video 1/1 (34/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 94.9ms\n",
      "video 1/1 (35/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 102.6ms\n",
      "video 1/1 (36/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 101.8ms\n",
      "video 1/1 (37/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 109.7ms\n",
      "video 1/1 (38/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 101.0ms\n",
      "video 1/1 (39/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 109.4ms\n",
      "video 1/1 (40/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 108.5ms\n",
      "video 1/1 (41/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 107.2ms\n",
      "video 1/1 (42/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 126.5ms\n",
      "video 1/1 (43/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 109.3ms\n",
      "video 1/1 (44/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 121.0ms\n",
      "video 1/1 (45/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 119.3ms\n",
      "video 1/1 (46/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 107.7ms\n",
      "video 1/1 (47/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 107.5ms\n",
      "video 1/1 (48/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 102.6ms\n",
      "video 1/1 (49/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 86.7ms\n",
      "video 1/1 (50/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 99.6ms\n",
      "video 1/1 (51/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 92.3ms\n",
      "video 1/1 (52/52) /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/1_video/video.mp4: 640x640 1 face, 2 hands, 94.6ms\n",
      "Speed: 0.8ms pre-process, 105.8ms inference, 1.1ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mrun_CNN_RNN/yolov5/runs/detect/exp\u001b[0m\n",
      "52 labels saved to run_CNN_RNN/yolov5/runs/detect/exp/labels\n",
      "\n",
      "52 frames extracted.\n",
      "Converted video_7.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_7.jpg\n",
      "Converted video_26.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_26.jpg\n",
      "Converted video_32.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_32.jpg\n",
      "Converted video_33.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_33.jpg\n",
      "Converted video_27.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_27.jpg\n",
      "Converted video_6.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_6.jpg\n",
      "Converted video_4.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_4.jpg\n",
      "Converted video_19.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_19.jpg\n",
      "Converted video_31.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_31.jpg\n",
      "Converted video_25.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_25.jpg\n",
      "Converted video_24.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_24.jpg\n",
      "Converted video_30.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_30.jpg\n",
      "Converted video_18.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_18.jpg\n",
      "Converted video_5.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_5.jpg\n",
      "Converted video_1.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_1.jpg\n",
      "Converted video_34.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_34.jpg\n",
      "Converted video_20.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_20.jpg\n",
      "Converted video_21.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_21.jpg\n",
      "Converted video_35.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_35.jpg\n",
      "Converted video_2.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_2.jpg\n",
      "Converted video_23.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_23.jpg\n",
      "Converted video_37.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_37.jpg\n",
      "Converted video_36.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_36.jpg\n",
      "Converted video_22.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_22.jpg\n",
      "Converted video_3.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_3.jpg\n",
      "Converted video_45.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_45.jpg\n",
      "Converted video_51.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_51.jpg\n",
      "Converted video_50.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_50.jpg\n",
      "Converted video_44.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_44.jpg\n",
      "Converted video_52.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_52.jpg\n",
      "Converted video_46.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_46.jpg\n",
      "Converted video_47.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_47.jpg\n",
      "Converted video_43.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_43.jpg\n",
      "Converted video_42.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_42.jpg\n",
      "Converted video_40.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_40.jpg\n",
      "Converted video_41.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_41.jpg\n",
      "Converted video_49.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_49.jpg\n",
      "Converted video_48.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_48.jpg\n",
      "Converted video_13.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_13.jpg\n",
      "Converted video_12.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_12.jpg\n",
      "Converted video_38.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_38.jpg\n",
      "Converted video_10.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_10.jpg\n",
      "Converted video_11.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_11.jpg\n",
      "Converted video_39.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_39.jpg\n",
      "Converted video_8.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_8.jpg\n",
      "Converted video_15.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_15.jpg\n",
      "Converted video_29.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_29.jpg\n",
      "Converted video_28.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_28.jpg\n",
      "Converted video_14.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_14.jpg\n",
      "Converted video_9.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_9.jpg\n",
      "Converted video_16.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_16.jpg\n",
      "Converted video_17.jpg to grayscale and saved to /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/2_convert_video_to_images/video_17.jpg\n",
      "Conversion completed.\n",
      "file '/Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/yolov5/runs/detect/exp/labels/video_1.txt' Successfully updated.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "best1: [0.283333 0.39213  0.153704 0.193519]\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "best1: [0.309722 0.4125   0.152778 0.175   ]\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "best1: [0.205093 0.404167 0.173148 0.199074]\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "All numbers 1, 2 and 3 are in the sixth position of the second file.\n",
      "Number of text files remaining in labels: 30\n",
      "Number of image files remaining in images: 30\n",
      "Resized and saved: video_1_1_1.jpg\n",
      "Resized and saved: video_16_3_0.jpg\n",
      "Resized and saved: video_8_3_0.jpg\n",
      "Resized and saved: video_30_1_1.jpg\n",
      "Resized and saved: video_20_1_1.jpg\n",
      "Resized and saved: video_15_2_1.jpg\n",
      "Resized and saved: video_29_3_0.jpg\n",
      "Resized and saved: video_1_3_0.jpg\n",
      "Resized and saved: video_8_1_1.jpg\n",
      "Resized and saved: video_23_2_1.jpg\n",
      "Resized and saved: video_16_1_1.jpg\n",
      "Resized and saved: video_20_3_0.jpg\n",
      "Resized and saved: video_30_3_0.jpg\n",
      "Resized and saved: video_2_2_1.jpg\n",
      "Resized and saved: video_29_1_1.jpg\n",
      "Resized and saved: video_17_1_1.jpg\n",
      "Resized and saved: video_9_1_1.jpg\n",
      "Resized and saved: video_22_2_1.jpg\n",
      "Resized and saved: video_21_3_0.jpg\n",
      "Resized and saved: video_3_2_1.jpg\n",
      "Resized and saved: video_28_1_1.jpg\n",
      "Resized and saved: video_9_3_0.jpg\n",
      "Resized and saved: video_17_3_0.jpg\n",
      "Resized and saved: video_14_2_1.jpg\n",
      "Resized and saved: video_21_1_1.jpg\n",
      "Resized and saved: video_28_3_0.jpg\n",
      "Resized and saved: video_15_1_1.jpg\n",
      "Resized and saved: video_30_2_1.jpg\n",
      "Resized and saved: video_20_2_1.jpg\n",
      "Resized and saved: video_2_3_0.jpg\n",
      "Resized and saved: video_1_2_1.jpg\n",
      "Resized and saved: video_23_3_0.jpg\n",
      "Resized and saved: video_15_3_0.jpg\n",
      "Resized and saved: video_29_2_1.jpg\n",
      "Resized and saved: video_2_1_1.jpg\n",
      "Resized and saved: video_16_2_1.jpg\n",
      "Resized and saved: video_23_1_1.jpg\n",
      "Resized and saved: video_8_2_1.jpg\n",
      "Resized and saved: video_14_3_0.jpg\n",
      "Resized and saved: video_28_2_1.jpg\n",
      "Resized and saved: video_3_1_1.jpg\n",
      "Resized and saved: video_22_1_1.jpg\n",
      "Resized and saved: video_9_2_1.jpg\n",
      "Resized and saved: video_17_2_1.jpg\n",
      "Resized and saved: video_21_2_1.jpg\n",
      "Resized and saved: video_14_1_1.jpg\n",
      "Resized and saved: video_3_3_0.jpg\n",
      "Resized and saved: video_22_3_0.jpg\n",
      "Resized and saved: video_5_2_1.jpg\n",
      "Resized and saved: video_27_3_0.jpg\n",
      "Resized and saved: video_24_2_1.jpg\n",
      "Resized and saved: video_11_1_1.jpg\n",
      "Resized and saved: video_18_3_0.jpg\n",
      "Resized and saved: video_6_3_0.jpg\n",
      "Resized and saved: video_27_1_1.jpg\n",
      "Resized and saved: video_12_2_1.jpg\n",
      "Resized and saved: video_11_3_0.jpg\n",
      "Resized and saved: video_6_1_1.jpg\n",
      "Resized and saved: video_18_1_1.jpg\n",
      "Resized and saved: video_13_2_1.jpg\n",
      "Resized and saved: video_26_1_1.jpg\n",
      "Resized and saved: video_10_3_0.jpg\n",
      "Resized and saved: video_19_1_1.jpg\n",
      "Resized and saved: video_7_1_1.jpg\n",
      "Resized and saved: video_4_2_1.jpg\n",
      "Resized and saved: video_26_3_0.jpg\n",
      "Resized and saved: video_10_1_1.jpg\n",
      "Resized and saved: video_25_2_1.jpg\n",
      "Resized and saved: video_7_3_0.jpg\n",
      "Resized and saved: video_19_3_0.jpg\n",
      "Resized and saved: video_11_2_1.jpg\n",
      "Resized and saved: video_24_1_1.jpg\n",
      "Resized and saved: video_5_1_1.jpg\n",
      "Resized and saved: video_12_3_0.jpg\n",
      "Resized and saved: video_24_3_0.jpg\n",
      "Resized and saved: video_18_2_1.jpg\n",
      "Resized and saved: video_6_2_1.jpg\n",
      "Resized and saved: video_5_3_0.jpg\n",
      "Resized and saved: video_12_1_1.jpg\n",
      "Resized and saved: video_27_2_1.jpg\n",
      "Resized and saved: video_25_3_0.jpg\n",
      "Resized and saved: video_7_2_1.jpg\n",
      "Resized and saved: video_19_2_1.jpg\n",
      "Resized and saved: video_4_3_0.jpg\n",
      "Resized and saved: video_26_2_1.jpg\n",
      "Resized and saved: video_13_1_1.jpg\n",
      "Resized and saved: video_25_1_1.jpg\n",
      "Resized and saved: video_10_2_1.jpg\n",
      "Resized and saved: video_4_1_1.jpg\n",
      "Resized and saved: video_13_3_0.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/1.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/16.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/8.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/30.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/20.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/15.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/29.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/23.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/2.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/17.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/9.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/22.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/21.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/3.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/28.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/14.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/5.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/27.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/24.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/11.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/18.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/6.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/12.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/13.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/26.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/10.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/19.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/7.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/4.jpg\n",
      "Image saved: /Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN/data/25.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from IPython.display import Image\n",
    "from random import choice\n",
    "import shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "import shutil\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "\n",
    "\n",
    "###path\n",
    "base_path = \"/Users/yayhaeslami/Desktop/my_workspace/sign_language/run_CNN_RNN\"\n",
    "\n",
    "all_video_input = f\"{base_path}/all_video\"\n",
    "one_video_input = f\"{all_video_input}/salam1.mp4\"\n",
    "video_folder = f\"{base_path}/1_video\"\n",
    "video_in_video_folder = f\"{video_folder}/video.mp4\"\n",
    "\n",
    "convert_video_to_images_folder = f\"{base_path}/2_convert_video_to_images\"\n",
    "square_images_folder = f\"{base_path}/3_square_images\"\n",
    "resized_images_folder = f\"{base_path}/4_resized_images\"\n",
    "data_end = f\"{base_path}/data\"\n",
    "\n",
    "yolo_folder = f\"{base_path}/yolov5\"\n",
    "detect_folder = f\"{yolo_folder}/runs/detect\"\n",
    "labels_folder = f\"{detect_folder}/exp/labels\"\n",
    "yolo_weights_path = f\"{yolo_folder}/runs/train/weights/best.pt\"\n",
    "\n",
    "model_path = f\"{base_path}/cnn_rnn_model.h5\"\n",
    "\n",
    "#name class\n",
    "name_class = \"bebakhshod\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±\n",
    "paths = [\n",
    "    video_folder,\n",
    "    convert_video_to_images_folder,\n",
    "    square_images_folder,\n",
    "    resized_images_folder,\n",
    "    yolo_weights_path,\n",
    "    data_end\n",
    "]\n",
    "\n",
    "# Ø§ÛŒØ¬Ø§Ø¯ ÙÙˆÙ„Ø¯Ø±Ù‡Ø§\n",
    "for path in paths:\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "        print(f\"Folder created: {path}\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Folder already exists: {path}\")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "### 1_ Deleted all files\n",
    "\n",
    "\n",
    "\n",
    "def delete_files_and_folders(pathsÙ€delete_files):\n",
    "    for folder_path in pathsÙ€delete_files:\n",
    "        items = glob.glob(os.path.join(folder_path, \"*\"))\n",
    "\n",
    "        for item in items:\n",
    "            try:\n",
    "                if os.path.isfile(item) or os.path.islink(item):\n",
    "                    os.remove(item)  \n",
    "            \n",
    "                elif os.path.isdir(item):\n",
    "                    shutil.rmtree(item)  \n",
    "                    print(f\"Deleted folder: {item}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {item}: {e}\")\n",
    "\n",
    "\n",
    "### 2_ cut up and down video and change name video\n",
    "\n",
    "def crop_video(video_path_change_name_video, save_path_change_name_video):\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_path_change_name_video)\n",
    "\n",
    "    width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    min_size = min(width, height)\n",
    "    start_x = (width - min_size) // 2\n",
    "    start_y = (height - min_size) // 2\n",
    "\n",
    "    crop_region = (start_x, start_y, min_size, min_size)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "    video_writer = cv2.VideoWriter(save_path_change_name_video, fourcc, 30, (min_size, min_size))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cropped_frame = frame[start_y:start_y+min_size, start_x:start_x+min_size]\n",
    "\n",
    "\n",
    "        video_writer.write(cropped_frame)\n",
    "\n",
    "\n",
    "    video_capture.release()\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3_ ran yolo model\n",
    "\n",
    "def run_yolov5_detection(video_source, weights_path, conf_threshold=0.4, vid_stride=1):\n",
    "    \"\"\"\n",
    "    Runs YOLOv5 object detection on a video source.\n",
    "\n",
    "    Parameters:\n",
    "        video_source (str): Path to the video file or directory containing images.\n",
    "        weights_path (str): Path to the YOLOv5 weights file.\n",
    "        conf_threshold (float, optional): Confidence threshold for detection. Defaults to 0.4.\n",
    "        vid_stride (int, optional): Frame stride for video processing. Defaults to 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(yolo_folder):\n",
    "        raise FileNotFoundError(f\"Directory {yolo_folder} does not exist\")\n",
    "    \n",
    "    # Construct the command with absolute paths\n",
    "    detect_script = os.path.join(yolo_folder, 'detect.py')\n",
    "    command = [\n",
    "        'python', detect_script,\n",
    "        '--source', video_source,\n",
    "        '--weights', weights_path,\n",
    "        '--save-txt',\n",
    "        '--vid-stride', str(vid_stride),\n",
    "        '--conf-thres', str(conf_threshold)\n",
    "    ]\n",
    "    \n",
    "    # Run the YOLOv5 detection script and capture output\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(\"Command Output:\", result.stdout)\n",
    "        print(\"Command Error:\", result.stderr)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error executing command:\")\n",
    "        print(e.output)\n",
    "        print(e.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "###4_ extract frames from video\n",
    "def convert_video_to_images(video_path, output_folder):\n",
    "    base_name = os.path.basename(video_path).split('.')[0]\n",
    "    \n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    frame_interval = 1 \n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    while success:\n",
    "        if count % frame_interval == 0:\n",
    "            image_path = os.path.join(output_folder, f\"{base_name}_{count // frame_interval + 1}.jpg\")\n",
    "            cv2.imwrite(image_path, image) \n",
    "        success, image = vidcap.read()\n",
    "        count += 1\n",
    "\n",
    "    print(f\"{count // frame_interval} frames extracted.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###5_ Convert photos to black and white\n",
    "\n",
    "def convert_photos_to_grayscale(input_folder):\n",
    "    \"\"\"\n",
    "    Converts photos in the input folder to grayscale and saves them in the output folder.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): The path to the folder containing the images.\n",
    "        output_folder (str): The path to the folder where the grayscale images will be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            img = cv2.imread(input_path)\n",
    "            \n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            cv2.imwrite(output_path, gray_img)\n",
    "            \n",
    "            print(f\"Converted {filename} to grayscale and saved to {output_path}\")\n",
    "\n",
    "    print(\"Conversion completed.\")\n",
    "\n",
    "\n",
    "### 6_ add 1,2,3 to ferst file .txt with Right hand and left hand detection\n",
    "\n",
    "def add_number_to_ferst_file(folder_path_labels):\n",
    "    file_pattern = os.path.join(folder_path_labels , \"*_1.txt\")\n",
    "    files = glob.glob(file_pattern)\n",
    "\n",
    "    if files:\n",
    "        file_path = files[0]\n",
    "        with open(file_path, 'r+') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        hands = []\n",
    "        others = []\n",
    "        for line in lines:\n",
    "            if line.startswith(\"1\"):\n",
    "                hands.append(line.strip())\n",
    "            else:\n",
    "                others.append(line.strip())\n",
    "        \n",
    "        hands_sorted = sorted(hands, key=lambda x: float(x.split()[1]))\n",
    "        \n",
    "        right_hand = \"\"\n",
    "        left_hand = \"\"\n",
    "\n",
    "        if len(hands_sorted) >= 1:\n",
    "            right_hand = hands_sorted[0] + \" 1\"\n",
    "        if len(hands_sorted) >= 2:\n",
    "            left_hand = hands_sorted[1] + \" 2\"\n",
    "\n",
    "        others = [coord + \" 3\" for coord in others]\n",
    "\n",
    "        result = []\n",
    "        if right_hand:\n",
    "            result.append(right_hand)\n",
    "        if left_hand:\n",
    "            result.append(left_hand)\n",
    "        result.extend(others)\n",
    "\n",
    "        with open(file_path, 'w') as f:\n",
    "            for line in result:\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "        print(f\"file '{file_path}' Successfully updated.\")\n",
    "    else:\n",
    "        print(\"No file with extension _1.txt was found in the desired path.\")\n",
    "\n",
    "\n",
    "\n",
    "######7_ Prepare text file for cut image\n",
    "\n",
    "def Prepare_text_file_for_cut_image(directory_path_labels_folder):\n",
    "\n",
    "    def extract_first_digit_after_decimal(line):\n",
    "        parts = line.strip().split()\n",
    "        first_digits_after_decimal = []\n",
    "        for part in parts[1:5]:\n",
    "            if '.' in part:\n",
    "                fraction_part = part.split('.')[1]\n",
    "                if len(fraction_part) > 0:\n",
    "                    first_digits_after_decimal.append(int(fraction_part[0]))\n",
    "                else:\n",
    "                    first_digits_after_decimal.append(0)\n",
    "            else:\n",
    "                first_digits_after_decimal.append(0)\n",
    "        return first_digits_after_decimal\n",
    "\n",
    "    def extract_second_digit_after_decimal(line):\n",
    "        parts = line.strip().split()\n",
    "        second_digits_after_decimal = []\n",
    "        for part in parts[1:5]:\n",
    "            if '.' in part:\n",
    "                fraction_part = part.split('.')[1]\n",
    "                if len(fraction_part) > 1:\n",
    "                    second_digits_after_decimal.append(int(fraction_part[1]))\n",
    "                else:\n",
    "                    second_digits_after_decimal.append(0)\n",
    "            else:\n",
    "                second_digits_after_decimal.append(0)\n",
    "        return second_digits_after_decimal\n",
    "\n",
    "    def count_matching_digits(list1, list2):\n",
    "        return sum(1 for a, b in zip(list1, list2) if a == b)\n",
    "\n",
    "    def read_file(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        return [list(map(float, line.strip().split())) for line in lines]\n",
    "\n",
    "    def write_file(file_path, lines):\n",
    "        with open(file_path, 'w') as file:\n",
    "            for line in lines:\n",
    "                formatted_line = ' '.join([f\"{int(num)}\" if num.is_integer() else f\"{num}\" for num in line])\n",
    "                file.write(formatted_line + '\\n')\n",
    "\n",
    "    def find_lines_with_sixth_element(lines, value):\n",
    "        return [line for line in lines if len(line) > 5 and line[5] == value]\n",
    "\n",
    "    def predict_next_coordinate(prev1, prev2):\n",
    "        diff = np.array(prev2) - np.array(prev1)\n",
    "        predicted = np.array(prev2) + diff\n",
    "        return predicted\n",
    "\n",
    "    def select_best_coordinate(predicted, candidates):\n",
    "        best_candidate = None\n",
    "        best_distance = float('inf')\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            candidate = np.array(candidate)\n",
    "            distance = np.linalg.norm(candidate - predicted)\n",
    "            \n",
    "            if distance < best_distance:\n",
    "                best_distance = distance\n",
    "                best_candidate = candidate\n",
    "        \n",
    "        return best_candidate\n",
    "\n",
    "    def process_files(file_path1, file_path2):\n",
    "        with open(file_path1, 'r') as file1, open(file_path2, 'r') as file2:\n",
    "            lines1 = file1.readlines()\n",
    "            lines2 = file2.readlines()\n",
    "\n",
    "        first_digits_list1 = [extract_first_digit_after_decimal(line) for line in lines1]\n",
    "        second_digits_list1 = [extract_second_digit_after_decimal(line) for line in lines1]\n",
    "\n",
    "        with open(file_path2, 'w') as file2:\n",
    "            for line2 in lines2:\n",
    "                first_digits2 = extract_first_digit_after_decimal(line2)\n",
    "                second_digits2 = extract_second_digit_after_decimal(line2)\n",
    "                matched_indices = []\n",
    "                matched = False\n",
    "\n",
    "                for i, first_digits1 in enumerate(first_digits_list1):\n",
    "                    if first_digits1 == first_digits2:\n",
    "                        matched_indices.append(i)\n",
    "\n",
    "                if len(matched_indices) == 1:\n",
    "                    parts = line2.strip().split()\n",
    "                    matched_line_parts = lines1[matched_indices[0]].strip().split()\n",
    "                    if len(matched_line_parts) > 5:\n",
    "                        parts.append(matched_line_parts[5])\n",
    "                    else:\n",
    "                        parts.append('')  # Leave the 6th number empty\n",
    "                    file2.write(' '.join(parts) + '\\n')\n",
    "                    matched = True\n",
    "                elif len(matched_indices) > 1:\n",
    "                    for i in matched_indices:\n",
    "                        if second_digits_list1[i] == second_digits2:\n",
    "                            parts = line2.strip().split()\n",
    "                            matched_line_parts = lines1[i].strip().split()\n",
    "                            if len(matched_line_parts) > 5:\n",
    "                                parts.append(matched_line_parts[5])\n",
    "                            else:\n",
    "                                parts.append('')  # Leave the 6th number empty\n",
    "                            file2.write(' '.join(parts) + '\\n')\n",
    "                            matched = True\n",
    "                            break\n",
    "\n",
    "                if not matched:\n",
    "                    for i, first_digits1 in enumerate(first_digits_list1):\n",
    "                        if count_matching_digits(first_digits1, first_digits2) >= 3:\n",
    "                            parts = line2.strip().split()\n",
    "                            matched_line_parts = lines1[i].strip().split()\n",
    "                            if len(matched_line_parts) > 5:\n",
    "                                parts.append(matched_line_parts[5])\n",
    "                            else:\n",
    "                                parts.append('')  # Leave the 6th number empty\n",
    "                            file2.write(' '.join(parts) + '\\n')\n",
    "                            matched = True\n",
    "                            break\n",
    "\n",
    "                if not matched:\n",
    "                    parts = line2.strip().split()\n",
    "                    parts.append('')  # Leave the 6th number empty\n",
    "                    file2.write(' '.join(parts) + '\\n')\n",
    "                    \n",
    "        file1_lines = read_file(file_path1)\n",
    "        file2_lines = read_file(file_path2)\n",
    "        \n",
    "        missing_values = [val for val in [1, 2, 3] if val not in [line[5] for line in file2_lines if len(line) > 5]]\n",
    "        \n",
    "        if missing_values:\n",
    "            missing_value = missing_values[0]  \n",
    "            \n",
    "            \n",
    "            file1_lines_with_missing = find_lines_with_sixth_element(file1_lines, missing_value)\n",
    "            \n",
    "            if file1_lines_with_missing:\n",
    "                \n",
    "                prev1 = file1_lines_with_missing[0][1:5]  \n",
    "                prev2 = file1_lines_with_missing[1][1:5] if len(file1_lines_with_missing) > 1 else prev1 \n",
    "                \n",
    "            \n",
    "                candidates = [line for line in file2_lines if len(line) <= 5 or (line[5] != 1 and line[5] != 2 and line[5] != 3)]\n",
    "                candidate_coords = [line[1:5] for line in candidates]\n",
    "                \n",
    "                if candidate_coords:\n",
    "                    \n",
    "                    predicted = predict_next_coordinate(prev1, prev2)\n",
    "                    \n",
    "                    \n",
    "                    best_coordinate = select_best_coordinate(predicted, candidate_coords)\n",
    "                    \n",
    "                    print(f\"best{missing_value}:\", best_coordinate)\n",
    "                    \n",
    "                    \n",
    "                    for line in candidates:\n",
    "                        if line[1:5] == best_coordinate.tolist():\n",
    "                            line.append(float(missing_value))\n",
    "                    \n",
    "                    \n",
    "                    write_file(file_path2, file2_lines)\n",
    "                else:\n",
    "                    print(\"There is no line without the sixth number or with the sixth number 1, 2, 3 in the second file.\")\n",
    "            else:\n",
    "                print(f\"No lines with numbers{missing_value} It is not present in the sixth position in the first file.\")\n",
    "        else:\n",
    "            print(\"All numbers 1, 2 and 3 are in the sixth position of the second file.\")\n",
    "\n",
    "    def process_files_in_directory(directory):\n",
    "        file_names = os.listdir(directory)\n",
    "        file_names = [f for f in file_names if f.endswith('.txt')]\n",
    "        file_names.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "\n",
    "        for i in range(len(file_names) - 1):\n",
    "            file_path1 = os.path.join(directory, file_names[i])\n",
    "            file_path2 = os.path.join(directory, file_names[i + 1])\n",
    "            process_files(file_path1, file_path2)\n",
    "\n",
    "    directory_path = directory_path_labels_folder\n",
    "    process_files_in_directory(directory_path)\n",
    "\n",
    "\n",
    "\n",
    "###8_ Deleted lines in files that haven't six number\n",
    "def Deleted_lines_in_files_that_haven_not_six_number(directory_path_labels_folder):\n",
    "\n",
    "    def read_file(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        return lines\n",
    "\n",
    "    def write_file(file_path, lines):\n",
    "        with open(file_path, 'w') as file:\n",
    "            for line in lines:\n",
    "                file.write(line)\n",
    "\n",
    "    def remove_lines_without_sixth_number(file_path):\n",
    "        lines = read_file(file_path)\n",
    "        filtered_lines = [line for line in lines if len(line.strip().split()) > 5]\n",
    "        write_file(file_path, filtered_lines)\n",
    "\n",
    "    def process_files_in_directory(directory):\n",
    "        file_names = os.listdir(directory)\n",
    "        file_names = [f for f in file_names if f.endswith('.txt')]\n",
    "\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            remove_lines_without_sixth_number(file_path)\n",
    "\n",
    "    directory_path = directory_path_labels_folder\n",
    "    process_files_in_directory(directory_path)\n",
    "\n",
    "\n",
    "\n",
    "###9_ Deleted file and image to 30\n",
    "def Deleted_file_and_image(labels_directory, images_directory):\n",
    "    label_files_before = set([f for f in os.listdir(labels_directory) if f.endswith('.txt')])\n",
    "\n",
    "    def get_file_number(filename):\n",
    "        return int(filename.split('_')[1].split('.')[0])\n",
    "\n",
    "    label_files_before = sorted(label_files_before, key=get_file_number)\n",
    "\n",
    "    def reduce_files_uniformly(files, target_count):\n",
    "        total_files = len(files)\n",
    "        if total_files <= target_count:\n",
    "            return files\n",
    "        \n",
    "        step = total_files / (total_files - target_count)\n",
    "        indices_to_remove = [int(i * step) for i in range(total_files - target_count)]\n",
    "        \n",
    "        for index in sorted(indices_to_remove, reverse=True):\n",
    "            os.remove(os.path.join(labels_directory, files[index]))\n",
    "        \n",
    "        remaining_files = [f for f in os.listdir(labels_directory) if f.endswith('.txt')]\n",
    "        remaining_files.sort(key=get_file_number)\n",
    "        return remaining_files\n",
    "\n",
    "    remaining_label_files = reduce_files_uniformly(label_files_before, 30)\n",
    "\n",
    "    deleted_label_files = set(label_files_before) - set(remaining_label_files)\n",
    "\n",
    "    for label_file in deleted_label_files:\n",
    "        image_file = label_file.replace('.txt', '.jpg')\n",
    "        image_path = os.path.join(images_directory, image_file)\n",
    "        if os.path.exists(image_path):\n",
    "            os.remove(image_path)\n",
    "\n",
    "    remaining_image_files = [f for f in os.listdir(images_directory) if f.endswith('.jpg')]\n",
    "    print(f\"Number of text files remaining in labels: {len(remaining_label_files)}\")\n",
    "    print(f\"Number of image files remaining in images: {len(remaining_image_files)}\")\n",
    "\n",
    "    def rename_files(directory, extension):\n",
    "        files = [f for f in os.listdir(directory) if f.endswith(extension)]\n",
    "        \n",
    "        files.sort(key=lambda f: int(re.search(r'_(\\d+)', f).group(1)))\n",
    "        \n",
    "        for i, filename in enumerate(files, start=1):\n",
    "            new_filename = re.sub(r'_\\d+', f'_{i}', filename)\n",
    "            os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n",
    "\n",
    "    rename_files(labels_directory, '.txt')\n",
    "\n",
    "    rename_files(images_directory, '.jpg')\n",
    "    rename_files(images_directory, '.png')\n",
    "    rename_files(images_directory, '.jpeg')\n",
    "\n",
    "\n",
    "###10_ cut image by text file\n",
    "\n",
    "def cut_image_by_text_file(labels_folder_path, images_folder_path, save_folder):\n",
    "    def convert_to_square(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            modified_lines = []\n",
    "            for line in lines:\n",
    "                label, x_center, y_center, width, height, additional_number = map(float, line.split())\n",
    "                \n",
    "                max_size = max(width, height)\n",
    "                \n",
    "                x1 = max(x_center - max_size / 2, 0)\n",
    "                y1 = max(y_center - max_size / 2, 0)\n",
    "                x2 = x1 + abs(max_size)\n",
    "                y2 = y1 + abs(max_size)\n",
    "                \n",
    "                modified_lines.append(f\"{int(label)} {x1} {y1} {x2} {y2} {int(additional_number)}\\n\")\n",
    "            \n",
    "            return modified_lines\n",
    "\n",
    "    def crop_and_save_squares(image_path, coordinates, save_folder):\n",
    "        image = cv2.imread(image_path)\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "        for i, (label, (x1, y1), (x2, y2), additional_number) in enumerate(coordinates):\n",
    "            x1_pixel = int(x1 * image.shape[1])\n",
    "            y1_pixel = int(y1 * image.shape[0])\n",
    "            x2_pixel = int(x2 * image.shape[1])\n",
    "            y2_pixel = int(y2 * image.shape[0])\n",
    "\n",
    "            cropped_image = image[y1_pixel:y2_pixel, x1_pixel:x2_pixel]\n",
    "\n",
    "            cropped_save_path = os.path.join(save_folder, f\"{image_name}_{additional_number}_{label}.jpg\")\n",
    "            cv2.imwrite(cropped_save_path, cropped_image)\n",
    "\n",
    "\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    file_list = os.listdir(labels_folder_path)\n",
    "\n",
    "    for file_name in file_list:\n",
    "        if file_name.endswith(\".txt\"): \n",
    "            text_file_path = os.path.join(labels_folder_path, file_name)\n",
    "            \n",
    "            modified_lines = convert_to_square(text_file_path)\n",
    "            \n",
    "            coordinates = []\n",
    "            for line in modified_lines:\n",
    "                label, x1, y1, x2, y2, additional_number = map(float, line.split())\n",
    "                coordinates.append((int(label), (x1, y1), (x2, y2), int(additional_number)))\n",
    "            \n",
    "            image_file_name = os.path.splitext(file_name)[0] + \".jpg\"\n",
    "            image_path = os.path.join(images_folder_path, image_file_name)\n",
    "            if os.path.exists(image_path):\n",
    "\n",
    "                crop_and_save_squares(image_path, coordinates, save_folder)\n",
    "            else:\n",
    "                print(f\"{file_name} not found.\")\n",
    "\n",
    "\n",
    "###11_ change Dimensions to 128 * 128\n",
    "\n",
    "def resize_images(input_folder_square_images_Dimensions, output_folder_resized_images_Dimensions, size):\n",
    "    if not os.path.exists(output_folder_resized_images_Dimensions):\n",
    "        os.makedirs(output_folder_resized_images_Dimensions)\n",
    "\n",
    "    for filename in os.listdir(input_folder_square_images_Dimensions):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            img_path = os.path.join(input_folder_square_images_Dimensions, filename)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is not None:\n",
    "                resized_image = cv2.resize(image, size)\n",
    "                \n",
    "                output_path = os.path.join(output_folder_resized_images_Dimensions, filename)\n",
    "                \n",
    "                cv2.imwrite(output_path, resized_image)\n",
    "                print(f\"Resized and saved: {filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to read: {filename}\")\n",
    "\n",
    "\n",
    "###12_ mix 3 image to one with LDA\n",
    "\n",
    "\n",
    "def mix_image_with_LDA(input_dir, output_dir, weights):\n",
    "    image_groups = defaultdict(list)\n",
    "\n",
    "    pattern = re.compile(r'^(.*?)_(\\d+)_(\\d+)')\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            class_name = match.group(1)\n",
    "            group_id = match.group(2)\n",
    "            weight_id = int(match.group(3))\n",
    "            image_groups[class_name].append((os.path.join(input_dir, filename), group_id, weight_id))\n",
    "\n",
    "    def load_image(image_path):\n",
    "        with Image.open(image_path) as img:\n",
    "            return np.array(img)\n",
    "\n",
    "    group_images = defaultdict(list)\n",
    "    \n",
    "    for class_name, image_paths_groups_weights in image_groups.items():\n",
    "        for image_path, group_id, weight_id in image_paths_groups_weights:\n",
    "            image = load_image(image_path)\n",
    "            weight = weights.get(weight_id, 1)\n",
    "            group_images[group_id].append((image, weight))\n",
    "\n",
    "    for group_id, images_weights in group_images.items():\n",
    "        combined_image = np.zeros_like(images_weights[0][0], dtype=np.float64)\n",
    "        total_weight = 0\n",
    "\n",
    "        for image, weight in images_weights:\n",
    "            combined_image += image * weight\n",
    "            total_weight += weight\n",
    "\n",
    "        if total_weight > 0:\n",
    "            combined_image /= total_weight\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"{group_id}.jpg\")\n",
    "        Image.fromarray(combined_image.astype(np.uint8)).save(output_path)\n",
    "\n",
    "        print(f\"Image saved: {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "pathsÙ€delete_files = [ \n",
    "    video_folder,\n",
    "    convert_video_to_images_folder,\n",
    "    square_images_folder,\n",
    "    resized_images_folder,\n",
    "    detect_folder,\n",
    "    data_end\n",
    "]\n",
    "\n",
    "delete_files_and_folders(pathsÙ€delete_files)\n",
    "\n",
    "\n",
    "#2\n",
    "video_path_change_name_video = one_video_input\n",
    "save_path_change_name_video = video_in_video_folder\n",
    "\n",
    "crop_video(video_path_change_name_video, save_path_change_name_video)\n",
    "\n",
    "\n",
    "#3\n",
    "\n",
    "\n",
    "run_yolov5_detection(\n",
    "    video_source=video_in_video_folder,\n",
    "    weights_path=yolo_weights_path,\n",
    "    conf_threshold=0.4,\n",
    "    vid_stride=1\n",
    ")\n",
    "\n",
    "#4\n",
    "video_path = video_in_video_folder\n",
    "output_folder = convert_video_to_images_folder\n",
    "\n",
    "convert_video_to_images(video_path, output_folder)\n",
    "\n",
    "\n",
    "#5\n",
    "input_folder = convert_video_to_images_folder\n",
    "\n",
    "convert_photos_to_grayscale(\n",
    "    input_folder\n",
    ")\n",
    "\n",
    "\n",
    "#6\n",
    "folder_path_labels = labels_folder\n",
    "\n",
    "add_number_to_ferst_file(folder_path_labels)\n",
    "\n",
    "\n",
    "#7\n",
    "directory_path_labels_folder = labels_folder\n",
    "\n",
    "Prepare_text_file_for_cut_image(directory_path_labels_folder)\n",
    "\n",
    "#8\n",
    "directory_path_labels_folder = labels_folder\n",
    "Deleted_lines_in_files_that_haven_not_six_number(directory_path_labels_folder)\n",
    "\n",
    "#9\n",
    "labels_directory = labels_folder\n",
    "images_directory = convert_video_to_images_folder\n",
    "\n",
    "Deleted_file_and_image(labels_directory, images_directory)\n",
    "\n",
    "#10\n",
    "labels_folder_path =  labels_folder\n",
    "images_folder_path = convert_video_to_images_folder\n",
    "save_folder = square_images_folder\n",
    "\n",
    "cut_image_by_text_file(labels_folder_path, images_folder_path, save_folder)\n",
    "\n",
    "#11\n",
    "input_folder_square_images_Dimensions = square_images_folder\n",
    "output_folder_resized_images_Dimensions = resized_images_folder\n",
    "size=(128, 128)\n",
    "resize_images(input_folder_square_images_Dimensions, output_folder_resized_images_Dimensions, size)\n",
    "\n",
    "#12\n",
    "input_dir = resized_images_folder\n",
    "output_base_dir = data_end\n",
    "weights = {\n",
    "  1: 0.5,\n",
    "  2: 0.5,  \n",
    "  3: 0.5   \n",
    "}\n",
    "\n",
    "mix_image_with_LDA(input_dir, output_base_dir, weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 375ms/step\n",
      "salam: 18.07%\n",
      "mamnon: 81.93%\n",
      "Predicted class: mamnon\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "\n",
    "\n",
    "frame_dir = data_end\n",
    "image_size = (125, 128)\n",
    "frames_per_sequence = 30\n",
    "\n",
    "\n",
    "\n",
    "model = load_model(model_path)\n",
    "input_shape = (frames_per_sequence, *image_size, 1)\n",
    "\n",
    "def load_frames(frame_dir):\n",
    "    frames = []\n",
    "    for frame_file in sorted(os.listdir(frame_dir)):\n",
    "        if frame_file.endswith('.jpg'):\n",
    "            frame_path = os.path.join(frame_dir, frame_file)\n",
    "            img = load_img(frame_path, target_size=image_size, color_mode='grayscale')\n",
    "            img_array = img_to_array(img)\n",
    "            frames.append(img_array)\n",
    "            if len(frames) == frames_per_sequence:\n",
    "                break\n",
    "    frames = np.array(frames)\n",
    "    frames = frames / 255.0  \n",
    "    frames = np.expand_dims(frames, axis=0)  \n",
    "    return frames\n",
    "\n",
    "\n",
    "frames = load_frames(frame_dir)\n",
    "\n",
    "predictions = model.predict(frames)\n",
    "\n",
    "class_names = ['salam', 'mamnon']\n",
    "predicted_probabilities = predictions[0]\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"{class_name}: {predicted_probabilities[i] * 100:.2f}%\")\n",
    "\n",
    "predicted_class = np.argmax(predicted_probabilities)\n",
    "print(f\"Predicted class: {class_names[predicted_class]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
